{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "import traceback\n",
        "from litellm import completion\n",
        "from typing import List, Dict, Callable, Any\n",
        "from dataclasses import dataclass, field"
      ],
      "metadata": {
        "id": "tek5k4D1oaax"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Prompt:\n",
        "\n",
        "  messages: List[Dict] = field(default_factory=list)\n",
        "  tools: List[Dict] = field(default_factory=list)\n",
        "  metadata: Dict = field(default_factory=dict)\n",
        "\n",
        "def generate_response(prompt: Prompt) -> str:\n",
        "\n",
        "  messages = prompt.messages\n",
        "  tools = prompt.tools\n",
        "  response = None\n",
        "\n",
        "  if not tools:\n",
        "    response = completion(\n",
        "        model = \"groq/llama-3.3-70b-versatile\",\n",
        "        messages = messages,\n",
        "        tools = tools,\n",
        "        max_tokens = 1024\n",
        "    )\n",
        "    return response['choices'][0]['message']['content']\n",
        "  else:\n",
        "    response = completion(\n",
        "        model = \"groq/llama-3.3-70b-versatile\",\n",
        "        messages = messages,\n",
        "        tools = tools,\n",
        "        max_tokens = 1024\n",
        "    )\n",
        "    if response.choices[0].message.tool_calls:\n",
        "      result = {\n",
        "          tool = response.choices[0].message.tool_calls[0]\n",
        "          args = json.loads(tool.function.arguments)\n",
        "          }\n",
        "      result = json.dumps(result)\n",
        "    else:\n",
        "      response.choices[0].message.content\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "GdXl5Sgka7sp",
        "outputId": "9f362476-7afd-48f2-db8c-da09a97dac92"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1518600987.py, line 16)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1518600987.py\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    model : \"groq/llama-3.3-70b-versatile\",\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3R2-Fw9OhTtJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}